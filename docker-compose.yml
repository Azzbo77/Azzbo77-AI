# docker-compose.yml
# This file defines a multi-service Docker setup for a local RAG AI agent.
# Services include Nginx (reverse proxy), Open WebUI (AI frontend), SearXNG (search engine),
# Ollama (AI model runtime), Redis (WebSocket support), Qdrant (vector database), and n8n (workflow automation).
# Each service has an explicit container_name for simpler management (e.g., 'docker logs ai-interface').

services:
  # Reverse Proxy (Nginx)
  # Routes external traffic to Open WebUI and n8n, handling SSL for secure HTTPS connections.
  reverse-proxy:
    container_name: reverse-proxy
    # Fixed name allows commands like 'docker logs reverse-proxy' without generated names.
    image: nginx:latest
    # Latest stable Nginx image ensures reliability, security updates, and performance.
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      # Mounts custom Nginx config (read-only) to define routing and SSL settings.
      - ./certs:/certs:ro
      # Mounts SSL certificates (read-only) for HTTPS support.
    ports:
      - "${NGINX_PORT:-443}:443"
      # Maps host port (default 443 from .env) to container port 443 for HTTPS access.
      # NGINX_PORT allows flexibility if 443 is in use.
    depends_on:
      ai-interface:
        condition: service_healthy
      # Starts only after Open WebUI is healthy to prevent routing errors.
    restart: unless-stopped
    # Restarts automatically unless manually stopped for high availability.
    networks:
      - Azzbo77-AI
      # Joins custom bridge network for secure communication with other services.
    logging:
      driver: "json-file"
      # Uses JSON logging for structured, parseable logs.
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        # Limits log size to 10MB (from .env) to manage disk space.
        max-file: "${LOG_MAX_FILES:-3}"
        # Keeps 3 log files for balance between storage and history.

  # AI Interface (Open WebUI)
  # User-facing web interface for AI model interaction, document uploads, web search, and RAG capabilities.
  ai-interface:
    container_name: ai-interface
    # Fixed name simplifies commands like 'docker exec -it ai-interface env'.
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG:-main}
    # Official Open WebUI image with configurable tag (defaults to 'main' if WEBUI_DOCKER_TAG is unset).
    environment:
      # Configures connections to Ollama, SearXNG, Redis, Qdrant, and enables RAG features.
      - OLLAMA_API_BASE_URL=${OLLAMA_API_BASE_URL:-http://model-runtime:11434}
      # URL for Ollama’s API to send AI model requests.
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://model-runtime:11434}
      # Base URL for Ollama, typically same as API URL, for model access.
      - ENABLE_RAG_WEB_SEARCH=${ENABLE_RAG_WEB_SEARCH:-true}
      # Enables Retrieval-Augmented Generation for web-enhanced responses.
      - RAG_WEB_SEARCH_ENGINE=${RAG_WEB_SEARCH_ENGINE:-searxng}
      # Sets SearXNG as the RAG search engine.
      - SEARXNG_QUERY_URL=${SEARXNG_QUERY_URL:-http://search-engine:8080/search?q=<query>&format=json}
      # SearXNG query URL for JSON-formatted search results.
      - RAG_WEB_SEARCH_RESULT_COUNT=${RAG_WEB_SEARCH_RESULT_COUNT:-5}
      # Limits search results to 5 per query (from .env).
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=${RAG_WEB_SEARCH_CONCURRENT_REQUESTS:-10}
      # Allows 10 simultaneous search requests (from .env).
      - ENABLE_WEBSOCKET_SUPPORT=${ENABLE_WEBSOCKET_SUPPORT:-true}
      # Activates WebSocket support for live AI responses.
      - WEBSOCKET_MANAGER=${WEBSOCKET_MANAGER:-redis}
      # Sets Redis as WebSocket manager.
      - WEBSOCKET_REDIS_URL=${WEBSOCKET_REDIS_URL:-redis://redis-valkey:6379/1}
      # Redis URL for WebSocket data.
      - LOG_LEVEL=DEBUG
      # Enables detailed logging to show WebSocket and RAG initialization.
      - ENABLE_DOCUMENT_UPLOAD=true
      # Enables document upload for PDFs, Excel, and other formats.
      - VECTOR_DATABASE_URL=http://qdrant:6333
      # Connects to Qdrant for storing document embeddings.
      - EMBEDDING_MODEL=bge-m3:latest
      # Specifies bge-m3 as the embedding model for document vectorization.
    depends_on:
      - model-runtime
      # Requires Ollama for AI model inference.
      - search-engine
      # Requires SearXNG for web search capabilities.
      - redis-valkey
      # Requires Redis for WebSocket real-time communication.
      - qdrant
      # Requires Qdrant for RAG vector storage.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      # Checks Open WebUI’s health endpoint to confirm it’s running.
      interval: 10s
      # Runs every 10 seconds.
      timeout: 5s
      # Fails if no response in 5 seconds.
      retries: 10
      # Retries 10 times (100s total) before marking unhealthy.
    restart: unless-stopped
    # Restarts unless manually stopped for reliability.
    volumes:
      - open-webui-data:/app/backend/data
      # Persists uploaded documents and Open WebUI settings.
    networks:
      - Azzbo77-AI
      # Connects to custom network for service communication.
    logging:
      driver: "json-file"
      # Matches logging settings for consistency.
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
      # 10MB, 3 files from .env for disk management.

  # Search Engine (SearXNG)
  # Privacy-focused search engine for Open WebUI’s RAG web search.
  search-engine:
    container_name: search-engine
    # Fixed name for commands like 'docker logs search-engine'.
    image: docker.io/searxng/searxng:latest
    # Latest SearXNG image for current search features.
    volumes:
      - ./searxng:/etc/searxng:rw
      # Mounts config directory (read-write) for customization.
    environment:
      - SEARXNG_BASE_URL=${SEARXNG_BASE_URL:-http://localhost:8080}
      # External base URL for browser access.
    restart: unless-stopped
    # Restarts unless manually stopped.
    networks:
      - Azzbo77-AI
      # Joins custom network for Open WebUI communication.
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
      # Matches logging settings.

  # Model Runtime (Ollama)
  # Runs AI models with GPU support for Open WebUI’s inference and document embeddings.
  model-runtime:
    container_name: model-runtime
    # Fixed name for commands like 'docker logs model-runtime'.
    image: ollama/ollama:latest
    # Latest Ollama image for up-to-date AI model execution.
    volumes:
      - ollama-data:/root/.ollama
      # Persists model data to retain models across restarts.
    runtime: nvidia
    # Enables NVIDIA GPU acceleration, assuming nvidia-docker is installed.
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # Makes all GPUs available to Ollama for performance.
      - OLLAMA_HOST=0.0.0.0
      # Allows Ollama to accept connections from other containers.
    command: serve
    # Starts Ollama server for API requests from Open WebUI and n8n.
    restart: unless-stopped
    # Restarts unless manually stopped.
    networks:
      - Azzbo77-AI
      # Connects to custom network for Open WebUI and n8n communication.
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
      # Consistent logging settings.

  # Redis for WebSocket Support
  # Manages real-time WebSocket connections for Open WebUI, enabling live updates.
  redis-valkey:
    container_name: redis-valkey
    # Fixed name for commands like 'docker exec -it redis-valkey valkey-cli ping'.
    image: docker.io/valkey/valkey:8.0.1-alpine
    # Valkey 8.0.1 (Redis-compatible) for lightweight WebSocket management.
    volumes:
      - redis-data:/data
      # Persists Redis data for WebSocket state continuity.
    command: "valkey-server --save 30 1"
    # Runs Valkey server, saving database every 30 minutes if a key changes.
    healthcheck:
      test: "[ $$(valkey-cli ping) = 'PONG' ]"
      # Ensures Redis responds with 'PONG' to confirm it’s running.
      start_period: 5s
      # Waits 5 seconds before first check for startup.
      interval: 1s
      # Checks every second for monitoring.
      timeout: 3s
      # Fails if no response in 3 seconds.
      retries: 5
      # Retries 5 times before marking unhealthy.
    restart: unless-stopped
    # Restarts unless stopped for WebSocket reliability.
    cap_drop:
      - ALL
      # Drops all Linux capabilities for security.
    cap_add:
      - SETGID
      # Allows group ID settings for Redis processes.
      - SETUID
      # Allows user ID settings for Redis processes.
      - DAC_OVERRIDE
      # Permits overriding file access for data persistence.
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-1m}"
        # Limits logs to 1MB, as Redis logs minimally.
        max-file: "${LOG_MAX_FILES:-1}"
        # One log file for Redis’s low log volume.
    networks:
      - Azzbo77-AI
      # Joins custom network for Open WebUI communication.

  # Vector Database (Qdrant)
  # Stores document embeddings for RAG, enabling efficient retrieval of relevant document chunks.
  qdrant:
    container_name: qdrant
    # Fixed name for commands like 'docker logs qdrant'.
    image: qdrant/qdrant:latest
    # Latest Qdrant image for robust vector storage and search.
    volumes:
      - qdrant-data:/qdrant/storage
      # Persists vector data to retain embeddings across restarts.
    ports:
      - "6333:6333"
      # Exposes port 6333 for internal communication (optional for debugging).
    restart: unless-stopped
    # Restarts unless manually stopped for reliability.
    networks:
      - Azzbo77-AI
      # Joins custom network for Open WebUI and n8n communication.
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
      # Matches logging settings for consistency.

  # Workflow Automation (n8n)
  # Automates document processing, vectorization, and integrations with external tools.
  n8n:
    container_name: n8n
    # Fixed name for commands like 'docker logs n8n'.
    image: n8nio/n8n:latest
    # Official n8n image for up-to-date workflow automation features.
    ports:
      - "5678:5678"
      # Exposes port 5678 for n8n’s web interface.
    volumes:
      - n8n-data:/home/node/.n8n
      # Persists n8n workflows and settings.
      - ./shared:/data/shared
      # Mounts shared directory for document processing (e.g., PDFs, Excel).
    environment:
      - N8N_HOST=0.0.0.0
      # Allows n8n to accept connections from other containers.
      - OLLAMA_HOST=http://model-runtime:11434
      # Connects to Ollama for embedding generation.
      - QDRANT_HOST=http://qdrant:6333
      # Connects to Qdrant for vector storage.
    depends_on:
      - model-runtime
      # Requires Ollama for AI model access.
      - qdrant
      # Requires Qdrant for vector operations.
    restart: unless-stopped
    # Restarts unless manually stopped for reliability.
    networks:
      - Azzbo77-AI
      # Joins custom network for communication with other services.
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
      # Matches logging settings for consistency.

# Persistent Volumes
# Named volumes to store data across container restarts.
volumes:
  ollama-data:
    # Stores Ollama’s AI model data for persistence.
  redis-data:
    # Stores Redis data for WebSocket state continuity.
  open-webui-data:
    # Stores Open WebUI’s uploaded documents and settings.
  qdrant-data:
    # Stores Qdrant’s vector data for document embeddings.
  n8n-data:
    # Stores n8n’s workflows and configuration.

# Networks
# Custom network for secure, isolated service communication.
networks:
  Azzbo77-AI:
    driver: bridge
    # Bridge network for single-host communication, ideal for this setup.
